{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/Segmentation-Model/blob/main/Diffusion_Models_for_Image_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Diffusion Models for Image Segmentation**\n",
        "\n",
        "This pipeline is based on MONAI tutorial [1] and use MONAI for 2D segmentation of images using DDPMs, as proposed in [2].\n",
        "\n",
        "The same structure can also be used for conditional image generation, or image-to-image translation, as proposed in [3,4].\n",
        "\n",
        "1. https://github.com/Project-MONAI/GenerativeModels/blob/main/tutorials/generative/image_to_image_translation/tutorial_segmentation_with_ddpm.ipynb\n",
        "\n",
        "2. Wolleb et al. \"Diffusion Models for Implicit Image Segmentation Ensembles\", https://arxiv.org/abs/2112.03145\n",
        "\n",
        "3. Waibel et al. \"A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images\", https://arxiv.org/abs/2208.14125\n",
        "\n",
        "4. Durrer et al. \"Diffusion Models for Contrast Harmonization of Magnetic Resonance Images\", https://aps.arxiv.org/abs/2303.08189"
      ],
      "metadata": {
        "id": "5kNsGKojDW1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/datasets/user164919/hutu-80"
      ],
      "metadata": {
        "id": "s5HY5J8jC7FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/https:/www.kaggle.com/datasets/user164919/hutu-80/hutu-80.zip"
      ],
      "metadata": {
        "id": "usxjykoJKYxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import monai\" || pip install -q \"monai[pillow, tqdm]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "!python -c \"import seaborn\" || pip install -q seaborn\n",
        "!python -c \"import generative\" ||pip install monai-generative"
      ],
      "metadata": {
        "id": "jjUg5UapBITb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from monai import transforms\n",
        "from monai.apps import DecathlonDataset\n",
        "from monai.config import print_config\n",
        "from monai.data import DataLoader, load_decathlon_datalist, CacheDataset\n",
        "from monai.utils import set_determinism\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "from generative.inferers import DiffusionInferer\n",
        "from generative.networks.nets.diffusion_model_unet import DiffusionModelUNet\n",
        "from generative.networks.schedulers.ddpm import DDPMScheduler\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
        "print_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TmuhDaHBb8D",
        "outputId": "a3de9a7f-2db9-4c48-a46f-a7b4beea822f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONAI version: 1.3.0\n",
            "Numpy version: 1.25.2\n",
            "Pytorch version: 2.2.1+cu121\n",
            "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
            "MONAI rev id: 865972f7a791bf7b42efbcd87c8402bd865b329e\n",
            "MONAI __file__: /usr/local/lib/python3.10/dist-packages/monai/__init__.py\n",
            "\n",
            "Optional dependencies:\n",
            "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "Nibabel version: 4.0.2\n",
            "scikit-image version: 0.19.3\n",
            "scipy version: 1.11.4\n",
            "Pillow version: 9.4.0\n",
            "Tensorboard version: 2.15.2\n",
            "gdown version: 4.7.3\n",
            "TorchVision version: 0.17.1+cu121\n",
            "tqdm version: 4.66.2\n",
            "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "psutil version: 5.9.5\n",
            "pandas version: 1.5.3\n",
            "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "transformers version: 4.38.2\n",
            "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
            "\n",
            "For details about installing the optional dependencies, please visit:\n",
            "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Microscopic Images Dataset of Human Duodenum Adenocarcinoma**"
      ],
      "metadata": {
        "id": "KWcMJRAPJUmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/MCF-7 cell populations Dataset\"\n",
        "\n",
        "os.listdir(data_dir)"
      ],
      "metadata": {
        "id": "q2EwuvMtCY5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbcf3a4-58b9-4f76-867a-9bae8ba91905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['images', 'masks']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Set deterministic training for reproducibility**"
      ],
      "metadata": {
        "id": "t5CA5aaYK6UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_determinism(1217)"
      ],
      "metadata": {
        "id": "cdJldf6sK5H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_descr_json = {\n",
        "    \"description\": \"HuTu-80 Dataset\",\n",
        "    \"name\": \"HuTu-80 Dataset\",\n",
        "    \"test\": [],\n",
        "    \"training\": [],\n",
        "    \"validation\": []\n",
        "}\n",
        "\n",
        "all_images = glob(os.path.join(data_dir, \"images\", \"*.png\"))\n",
        "print(\"Total Images :::\", len(all_images))\n",
        "\n",
        "train_images, test_images = train_test_split(all_images, test_size = 0.2, random_state = 1217)\n",
        "\n",
        "print(\"Train subset ::\", len(train_images), \"\\nTest subset ::\", len(test_images))\n",
        "\n",
        "#dataset_descr_json[\"training\"]\n",
        "\n",
        "for image in tqdm(train_images):\n",
        "    image_paths = {}\n",
        "    image_paths[\"image\"] = image\n",
        "    image_paths[\"label\"] = image.replace(\"images\", \"masks\")\n",
        "    dataset_descr_json[\"training\"].append(image_paths)\n",
        "\n",
        "\n",
        "for image in tqdm(test_images):\n",
        "    image_paths = {}\n",
        "    image_paths[\"image\"] = image\n",
        "    image_paths[\"label\"] = image.replace(\"images\", \"masks\")\n",
        "    dataset_descr_json[\"validation\"].append(image_paths)\n",
        "\n",
        "\n",
        "print()\n",
        "print(f\"dataset_descr_json[training]: {len(dataset_descr_json['training'])}\")\n",
        "print(f\"dataset_descr_json[validation]: {len(dataset_descr_json['validation'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkM-WvaTK_ve",
        "outputId": "b2a99ae9-a00d-4724-b4c0-ae4e2032225d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Images ::: 180\n",
            "Train subset :: 144 \n",
            "Test subset :: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 144/144 [00:00<00:00, 451404.91it/s]\n",
            "100%|██████████| 36/36 [00:00<00:00, 244724.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dataset_descr_json[training]: 144\n",
            "dataset_descr_json[validation]: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "split_json = \"./dataset_0.json\"\n",
        "\n",
        "with open(split_json, 'w') as f:\n",
        "    json.dump(dataset_descr_json, f)"
      ],
      "metadata": {
        "id": "NcPdm59p9AE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocessing of the HuTu Dataset for training**"
      ],
      "metadata": {
        "id": "xzqy-Qxc_VrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
        "        transforms.EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        transforms.Lambdad(keys=[\"label\"], func=lambda x: np.where(x[0, :, :].unsqueeze(0) > 127, 1, 0), overwrite=True),\n",
        "        transforms.EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        transforms.Resized(keys=[\"image\", \"label\"], spatial_size =(512, 512), mode=[\"bilinear\",\"nearest\"]),\n",
        "        transforms.ScaleIntensityRangePercentilesd(keys=\"image\", lower=0, upper=99.5, b_min=0, b_max=1),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "e_FEHU6f_Nu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = load_decathlon_datalist(split_json, True, \"training\")\n",
        "train_ds = CacheDataset(\n",
        "    data=train_files,\n",
        "    transform=train_transforms,\n",
        "#              cache_num=1, # Uncomment it for debug purposes\n",
        "    cache_rate=1.0,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btw1Y2sW_07_",
        "outputId": "d70275c7-70db-4f23-91dc-29e0a2436d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|██████████| 144/144 [00:52<00:00,  2.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of training data: {len(train_ds)}\")  # this gives the number of samples in the training set\n",
        "print(f'Train image shape {train_ds[0][\"image\"].shape}')\n",
        "print(f'Train label shape {train_ds[0][\"label\"].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkC6gvWVAFmA",
        "outputId": "18426a99-7160-4120-a55d-22934669e537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training data: 144\n",
            "Train image shape torch.Size([3, 512, 512])\n",
            "Train label shape torch.Size([1, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True, persistent_workers=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umA6c-opAQkO",
        "outputId": "03e7fec8-dae8-4ca9-9fe5-166048c3d183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocessing of the Dataset for validation**"
      ],
      "metadata": {
        "id": "iKWowXgi_tqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_files = load_decathlon_datalist(split_json, True, \"validation\")"
      ],
      "metadata": {
        "id": "RhDtJSZyAZrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = CacheDataset(\n",
        "    data=val_files,\n",
        "    transform=train_transforms,\n",
        "#             cache_num=4,  # Uncomment it for debug purposes\n",
        "    cache_rate=1.0,\n",
        "    num_workers=4,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdoxxkDLAn1G",
        "outputId": "f659c7bc-4b36-48b9-c495-3cb504a45544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|██████████| 36/36 [00:11<00:00,  3.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of training data: {len(val_ds)}\")\n",
        "print(f'Validation Image shape {val_ds[0][\"image\"].shape}')\n",
        "print(f'Validation Label shape {val_ds[0][\"label\"].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E53J0MAAtyo",
        "outputId": "7d03f543-b6c1-41a8-efd1-80e62040f84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training data: 36\n",
            "Validation Image shape torch.Size([3, 512, 512])\n",
            "Validation Label shape torch.Size([1, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=True, persistent_workers=True\n",
        ")"
      ],
      "metadata": {
        "id": "z1UmBl9CAxLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define network, scheduler, optimizer, and inferer**\n",
        "\n",
        "At this step, we instantiate the MONAI components to create a DDPM, the UNET, the noise scheduler, and the inferer used for training and sampling.\n",
        "\n",
        "We are using the DDPM scheduler containing 100 timesteps, and a 2D UNET with attention mechanisms in the 3rd level (num_head_channels=64)."
      ],
      "metadata": {
        "id": "fny9Kr1EBo7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "_NWPEjYlBDW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionModelUNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=4,\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 64, 64),\n",
        "    attention_levels=(False, False, True),\n",
        "    num_res_blocks=1,\n",
        "    num_head_channels=64,\n",
        "    with_conditioning=False,\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJEzx_HCSh9",
        "outputId": "76b85412-5585-4943-de87-4394fb88bad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DiffusionModelUNet(\n",
              "  (conv_in): Convolution(\n",
              "    (conv): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              "  (time_embed): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "    (1): SiLU()\n",
              "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  )\n",
              "  (down_blocks): ModuleList(\n",
              "    (0-1): 2 x DownBlock(\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv1): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (conv2): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "      )\n",
              "      (downsampler): Downsample(\n",
              "        (op): Convolution(\n",
              "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): AttnDownBlock(\n",
              "      (attentions): ModuleList(\n",
              "        (0): AttentionBlock(\n",
              "          (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (proj_attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (resnets): ModuleList(\n",
              "        (0): ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv1): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (conv2): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (middle_block): AttnMidBlock(\n",
              "    (resnet_1): ResnetBlock(\n",
              "      (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "      (nonlinearity): SiLU()\n",
              "      (conv1): Convolution(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "      (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "      (conv2): Convolution(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (skip_connection): Identity()\n",
              "    )\n",
              "    (attention): AttentionBlock(\n",
              "      (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "      (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (proj_attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "    )\n",
              "    (resnet_2): ResnetBlock(\n",
              "      (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "      (nonlinearity): SiLU()\n",
              "      (conv1): Convolution(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "      (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "      (conv2): Convolution(\n",
              "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (skip_connection): Identity()\n",
              "    )\n",
              "  )\n",
              "  (up_blocks): ModuleList(\n",
              "    (0): AttnUpBlock(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv1): Convolution(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (conv2): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Convolution(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (attentions): ModuleList(\n",
              "        (0-1): 2 x AttentionBlock(\n",
              "          (norm): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (proj_attn): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (upsampler): Upsample(\n",
              "        (conv): Convolution(\n",
              "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): UpBlock(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv1): Convolution(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (conv2): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Convolution(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (upsampler): Upsample(\n",
              "        (conv): Convolution(\n",
              "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): UpBlock(\n",
              "      (resnets): ModuleList(\n",
              "        (0-1): 2 x ResnetBlock(\n",
              "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
              "          (nonlinearity): SiLU()\n",
              "          (conv1): Convolution(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (time_emb_proj): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "          (conv2): Convolution(\n",
              "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Convolution(\n",
              "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
              "    (1): SiLU()\n",
              "    (2): Convolution(\n",
              "      (conv): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model training of the Diffusion Model**\n",
        "We train our diffusion model for 100 epochs.\n",
        "\n",
        "In every step, we concatenate the original microscopic image to the noisy segmentation mask, to predict a slightly denoised segmentation mask.\n",
        "\n",
        "This is described in Equation 7 of the paper https://arxiv.org/pdf/2112.03145.pdf."
      ],
      "metadata": {
        "id": "BLgYCMqqD4Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "n_timesteps=10\n",
        "val_interval = 5\n",
        "epoch_loss_list = []\n",
        "val_epoch_loss_list = []"
      ],
      "metadata": {
        "id": "HM12qglTDm2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = DDPMScheduler(num_train_timesteps=n_timesteps)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
        "inferer = DiffusionInferer(scheduler)"
      ],
      "metadata": {
        "id": "FkcK28ZEEORv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = GradScaler()\n",
        "total_start = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, data in enumerate(train_loader):\n",
        "        images = data[\"image\"].to(device)\n",
        "        seg = data[\"label\"].to(device)  # this is the ground truth segmentation\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        timesteps = torch.randint(0, n_timesteps, (len(images),)).to(device)  # pick a random time step t\n",
        "\n",
        "        with autocast(enabled=True):\n",
        "            # Generate random noise\n",
        "            noise = torch.randn_like(seg).to(device)\n",
        "            noisy_seg = scheduler.add_noise(\n",
        "                original_samples=seg, noise=noise, timesteps=timesteps\n",
        "            )  # we only add noise to the segmentation mask\n",
        "            combined = torch.cat(\n",
        "                (images, noisy_seg), dim=1\n",
        "            )  # we concatenate the microscopic image with the noisy segmenatation mask, to condition the generation process\n",
        "            prediction = model(x=combined, timesteps=timesteps)\n",
        "            # Get model prediction\n",
        "            loss = F.mse_loss(prediction.float(), noise.float())\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
        "    if (epoch) % val_interval == 0:\n",
        "        model.eval()\n",
        "        val_epoch_loss = 0\n",
        "        for step, data_val in enumerate(val_loader):\n",
        "            images = data_val[\"image\"].to(device)\n",
        "            seg = data_val[\"label\"].to(device)  # this is the ground truth segmentation\n",
        "            timesteps = torch.randint(0, n_timesteps, (len(images),)).to(device)\n",
        "            with torch.no_grad():\n",
        "                with autocast(enabled=True):\n",
        "                    noise = torch.randn_like(seg).to(device)\n",
        "                    noisy_seg = scheduler.add_noise(original_samples=seg, noise=noise, timesteps=timesteps)\n",
        "                    combined = torch.cat((images, noisy_seg), dim=1)\n",
        "                    prediction = model(x=combined, timesteps=timesteps)\n",
        "                    val_loss = F.mse_loss(prediction.float(), noise.float())\n",
        "            val_epoch_loss += val_loss.item()\n",
        "        print(\"Epoch\", epoch, \"Validation loss\", val_epoch_loss / (step + 1))\n",
        "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
        "\n",
        "torch.save(model.state_dict(), \"./segmodel.pt\")\n",
        "total_time = time.time() - total_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJAJSaj8FmrL",
        "outputId": "d1b41c42-28f7-445f-e65d-66290eaf224f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Validation loss 0.46243801216284436\n",
            "Epoch 5 Validation loss 0.03852957238753637\n",
            "Epoch 10 Validation loss 0.026561259395546384\n",
            "Epoch 15 Validation loss 0.03044199488229222\n",
            "Epoch 20 Validation loss 0.017325865984376933\n",
            "Epoch 25 Validation loss 0.014769794484083023\n",
            "Epoch 30 Validation loss 0.014738088137366705\n",
            "Epoch 35 Validation loss 0.0114727019228869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train diffusion completed, total time: {total_time}.\")\n",
        "plt.title(\"Learning Curves Diffusion Model\", fontsize=20)\n",
        "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, color=\"red\", linewidth=2.0, label=\"Train\")\n",
        "plt.plot(\n",
        "    np.linspace(val_interval, n_epochs, int(n_epochs / val_interval)),\n",
        "    val_epoch_loss_list,\n",
        "    color=\"green\",\n",
        "    linewidth=2.0,\n",
        "    label=\"Validation\",\n",
        ")\n",
        "plt.yticks(fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.xlabel(\"Epochs\", fontsize=16)\n",
        "plt.ylabel(\"Loss\", fontsize=16)\n",
        "plt.legend(prop={\"size\": 14})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eBUyU_frF9YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Sampling of a new segmentation mask for an input image of the validation set**\n",
        "\n",
        "Starting from random noise, we want to generate a segmentation mask for a microscopic image of our validation set.\n",
        "\n",
        "Due to the stochastic generation process, we can sample an ensemble of n different segmentation masks per image.\n",
        "\n",
        "First, we pick an image of our validation set, and check the ground truth segmentation mask."
      ],
      "metadata": {
        "id": "scmIVE64GPDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "data = val_ds[idx]\n",
        "inputimg = data[\"image\"]  # Pick an input slice of the validation set to be segmented\n",
        "inputlabel = data[\"label\"]  # Check out the ground truth label mask.\n",
        "\n",
        "\n",
        "plt.figure(\"input\" + str(inputlabel))\n",
        "plt.imshow(inputimg.T, vmin=0, vmax=1, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(\"input\" + str(inputlabel))\n",
        "plt.imshow(inputlabel.T, vmin=0, vmax=1, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "HNHRAKqCGGGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we set the number of samples in the ensemble n. Starting from the input image (which ist the microscopic image), we follow Algorithm 1 of the paper \"Diffusion Models for Implicit Image Segmentation Ensembles\" (https://arxiv.org/pdf/2112.03145.pdf) n times. This gives us an ensemble of n different predicted segmentation masks."
      ],
      "metadata": {
        "id": "gUJlRuKWG0cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 5\n",
        "input_img = inputimg[None, ...].to(device)\n",
        "\n",
        "ensemble = []\n",
        "for k in range(5):\n",
        "    noise_shape=list(input_img.shape)\n",
        "    noise_shape[1] = 1\n",
        "    noise = torch.randn(noise_shape).to(device)\n",
        "    current_img = noise  # for the segmentation mask, we start from random noise.\n",
        "    combined = torch.cat(\n",
        "        (input_img, noise), dim=1\n",
        "    )  # We concatenate the input microscopic image to add spartial information.\n",
        "    scheduler.set_timesteps(num_inference_steps=n_timesteps)\n",
        "    progress_bar = tqdm(scheduler.timesteps)\n",
        "    chain = torch.zeros(current_img.shape)\n",
        "    for t in progress_bar:  # go through the noising process\n",
        "        with autocast(enabled=False):\n",
        "            with torch.no_grad():\n",
        "                model_output = model(combined, timesteps=torch.Tensor((t,)).to(current_img.device))\n",
        "                current_img, _ = scheduler.step(\n",
        "                    model_output, t, current_img\n",
        "                )  # this is the prediction x_t at the time step t\n",
        "                if t % (n_timesteps//10) == 0:\n",
        "                    chain = torch.cat((chain, current_img.cpu()), dim=-1)\n",
        "                combined = torch.cat(\n",
        "                    (input_img, current_img), dim=1\n",
        "                )  # in every step during the denoising process, the microscopic image is concatenated to add spartial information\n",
        "\n",
        "    plt.style.use(\"default\")\n",
        "    plt.imshow(chain[0, 0, ..., 512:].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    plt.tight_layout()\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    ensemble.append(current_img)  # this is the output of the diffusion model after T=n_timesteps denoising steps"
      ],
      "metadata": {
        "id": "jKf2azyZG16W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Segmentation prediction**\n",
        "\n",
        "The predicted segmentation mask is obtained from the output of the diffusion model by thresholding.\n",
        "\n",
        "We compute the Dice score for all predicted segmentations of the ensemble, as well as the pixel-wise mean and the variance map over the ensemble. As shown in the paper \"Diffusion Models for Implicit Image Segmentation Ensembles\" (https://arxiv.org/abs/2112.03145), we see that taking the mean over n=5 samples improves the segmentation performance.\n",
        "\n",
        "The variance maps highlights pixels where the model is unsure about it's own prediction."
      ],
      "metadata": {
        "id": "Ulp0QQfLHSSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coeff(im1, im2, empty_score=1.0):\n",
        "    im1 = np.asarray(im1).astype(bool)\n",
        "    im2 = np.asarray(im2).astype(bool)\n",
        "\n",
        "    im_sum = im1.sum() + im2.sum()\n",
        "    if im_sum == 0:\n",
        "        return empty_score\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(im1, im2)\n",
        "\n",
        "    return 2.0 * intersection.sum() / im_sum"
      ],
      "metadata": {
        "id": "KDuz2x63IJQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(ensemble)):\n",
        "    prediction = torch.where(ensemble[i] > 0.5, 1, 0).float()  # a binary mask is obtained via thresholding\n",
        "    score = dice_coeff(\n",
        "        prediction[0, 0].cpu(), inputlabel.cpu()\n",
        "    )  # we compute the dice scores for all samples separately\n",
        "    print(\"Dice score of sample\" + str(i), score)\n",
        "\n",
        "\n",
        "E = torch.where(torch.cat(ensemble) > 0.5, 1, 0).float()\n",
        "var = torch.var(E, dim=0)  # pixel-wise variance map over the ensemble\n",
        "mean = torch.mean(E, dim=0)  # pixel-wise mean map over the ensemble\n",
        "mean_prediction = torch.where(mean > 0.5, 1, 0).float()\n",
        "\n",
        "score = dice_coeff(mean_prediction[0, ...].cpu(), inputlabel.cpu())  # Here we predict the Dice score for the mean map\n",
        "print(\"Dice score on the mean map\", score)\n",
        "\n",
        "plt.style.use(\"default\")\n",
        "plt.imshow(mean[0, ...].cpu(), vmin=0, vmax=1, cmap=\"gray\")  # We plot the mean map\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.style.use(\"default\")\n",
        "plt.imshow(var[0, ...].cpu(), vmin=0, vmax=1, cmap=\"jet\")  # We plot the variance map\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5uvuadMOIOiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}